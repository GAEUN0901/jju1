{
    "MAP": "function ˆf(x, y)estimates the density at any location (x, y)\nbased on neighboring points:\nˆf(x, y) =1\nnhxhynX\ni=1K\u0012x−xi\nhx,y−yi\nhy\u0013\n(5)\nK(·,·)is the kernel function, typically Gaussian:\nK((x, y),(x′, y′)) = exp\u0010\n−(x−x′)2+(y−y′)2\n2σ2\u0011\n(6)\nWhere (x, y)and(x′, y′)are two two-dimensional data\npoints, hx,hyandσare bandwidth parameters that con-\ntrol the smoothness in the x direction, y direction and kernel\nrespectively. The KDE visualization highlights distribution",
    "hours. The overall runtime for AITP training is approxi-": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "mately 952 GPU hours.": "consistent with the full pre-training corpus. The reservoir\nsampling algorithm is described in the Appendix B.\nWe conduct experiments on the NVIDIA A800-SXM4-\n80GB, with the difference set generation phase taking ap-\nproximately 56 GPU hours. The Data Transformation Set-\nting phase utilizes the vLLM (Kwon et al., 2023) frame-\nwork to accelerate inference, requiring approximately 640\nGPU hours, while the Training Setting phase, involving\nfull-parameter fine-tuning, takes approximately 256 GPU\n3",
    "3.3. Ablation Study": "uses 2k degrees of freedom instead of k-1 or 2k-\n1. This choice is based on ……\nOriginal SFT Dataset\nCombined SetPretraining Corpus Difference Set\nRewritten SetRewriting\nSupervised \nFine-TuningRaw Text\nText: # Confidence Interval calculation for \nPower Density Estimation in MATLAB\nFirst of all, I am new to these statistics stuff but \nvery interested in the background. I try to……\nOriginal SFT Dataset",
    "In this section, we conduct an ablation study to assess the": "Pre-training (AITP), a method that systematically bridges\nthis gap. Rather than generating instruction-response pairs\nfrom scratch, AITP identifies gaps in existing datasets by\ncomparing their distribution to that of the pre-training cor-\npus. Underrepresented data is then rewritten into high-\nquality instruction-response pairs, enhancing dataset cover-\nage and alignment. As shown in Figure 2, AITP involves\nthree stages: (1) generating a difference set based on density",
    "effectiveness of adaptive data selection, controlled rewrit-": "the model in generating responses to the remaining high-\nquality queries.\n2.3. Training\nIn this phase, the model is trained on a combined dataset that\nincludes both the rewritten data derived from the difference\nset and the original SFT dataset. Notably, the model trained\non the combined dataset is the same as the one trained on the\npre-training corpus. This serves two main purposes: first, it\nensures consistency between the supplemented knowledge",
    "ing, and balanced integration. We also evaluate AITP with": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "varying threshold values.": "regions in the pre-training data distribution that are absent\nfrom or sparsely populated in the supervised fine-tuning\n(SFT) data. This can be formalized as follows:\nDdiff={di|di∈Dpretrain,∆(di, DSFT)< τ} (1)\nWhere Dpretrain ,DSFT,Ddiffrepresent the pre-training\ndataset, the SFT dataset and the resulting difference set,\nrespectively. ∆(di, DSFT)represents the density estimate of\nthe data point diin the SFT dataset, and τis the threshold\nthat determines whether a data point should be included in",
    "3.3.1. Adaptive Data Selection": "regions in the pre-training data distribution that are absent\nfrom or sparsely populated in the supervised fine-tuning\n(SFT) data. This can be formalized as follows:\nDdiff={di|di∈Dpretrain,∆(di, DSFT)< τ} (1)\nWhere Dpretrain ,DSFT,Ddiffrepresent the pre-training\ndataset, the SFT dataset and the resulting difference set,\nrespectively. ∆(di, DSFT)represents the density estimate of\nthe data point diin the SFT dataset, and τis the threshold\nthat determines whether a data point should be included in",
    "We compare adaptive data selection with random sampling,": "regions in the pre-training data distribution that are absent\nfrom or sparsely populated in the supervised fine-tuning\n(SFT) data. This can be formalized as follows:\nDdiff={di|di∈Dpretrain,∆(di, DSFT)< τ} (1)\nWhere Dpretrain ,DSFT,Ddiffrepresent the pre-training\ndataset, the SFT dataset and the resulting difference set,\nrespectively. ∆(di, DSFT)represents the density estimate of\nthe data point diin the SFT dataset, and τis the threshold\nthat determines whether a data point should be included in",
    "a commonly used method in other instruction-tuning meth-": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "ods. Specifically, we compare the effectiveness of AITP and": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "AITP-RS, where AITP-RS is the AITP method using data": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "from random sampling instead of adaptive data selection.": "(Wang et al., 2022b; Zhou et al., 2023a). Synthetic datasets,\non the other hand, frequently depend on expensive APIs\n*Equal contribution1School of Artificial Intelligence, Univer-\nsity of Chinese Academy of Sciences2Institute of Automation,\nChinese Academy of Sciences3BAAI4M-A-P501.ai6Peking Uni-\nversity. Correspondence to: Ge Zhang <gezhang@umich.edu >,\nJiaheng Liu <buaaljiaheng@gmail.com >, wenhaohuang <wen-\nhaohuang@xxx.edu >, JiaJun Zhang <jjzhang@nlpr.ia.ac.cn >.\nPreprint. Work in Progress",
    "3.3.2. Controlled Rewriting": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "We compare controlled rewriting (CW) with uncontrolled": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "rewriting (UCW) by evaluating the effectiveness of AITP": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "and AITP-UCW, where AITP-UCW is the AITP method": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-",
    "without query scoring.": "the model in generating responses to the remaining high-\nquality queries.\n2.3. Training\nIn this phase, the model is trained on a combined dataset that\nincludes both the rewritten data derived from the difference\nset and the original SFT dataset. Notably, the model trained\non the combined dataset is the same as the one trained on the\npre-training corpus. This serves two main purposes: first, it\nensures consistency between the supplemented knowledge",
    "3.3.3. Balanced Integration": "uses 2k degrees of freedom instead of k-1 or 2k-\n1. This choice is based on ……\nOriginal SFT Dataset\nCombined SetPretraining Corpus Difference Set\nRewritten SetRewriting\nSupervised \nFine-TuningRaw Text\nText: # Confidence Interval calculation for \nPower Density Estimation in MATLAB\nFirst of all, I am new to these statistics stuff but \nvery interested in the background. I try to……\nOriginal SFT Dataset",
    "We compare balanced integration (BI) with unbalanced in-": "regions in the pre-training data distribution that are absent\nfrom or sparsely populated in the supervised fine-tuning\n(SFT) data. This can be formalized as follows:\nDdiff={di|di∈Dpretrain,∆(di, DSFT)< τ} (1)\nWhere Dpretrain ,DSFT,Ddiffrepresent the pre-training\ndataset, the SFT dataset and the resulting difference set,\nrespectively. ∆(di, DSFT)represents the density estimate of\nthe data point diin the SFT dataset, and τis the threshold\nthat determines whether a data point should be included in",
    "tegration (UBI) by evaluating the effectiveness of AITP and": "not only release model weights but also training datasets and\nintermediate checkpoints, aiming to facilitate reproduction\nand advance scientific research in LLMs. In this paper,\nthe OLMo-7B-base, MAP-Neo-7B-base, and Pythia-12B\nmodels, along with their corresponding pre-training corpora,\nare chosen as the foundational setup for AITP. The OLMo-\n7B-SFT and MAP-Neo-7B-SFT-v0.1 models are used as\nbaselines to validate the effectiveness of AITP. Since the",
    "AITP-UBI, where": "Original SFT Dataset\nFigure 2: The pipeline of AITP. AITP first generates a difference set, then rewrites the raw text into instruction-response\npairs to form a rewritten set, and finally combines the rewritten set with the original SFT dataset for model training.\npairs, and (3) integrating these pairs into the original dataset\nfor fine-tuning.\nFigure 1 visualizes the significant distributional differences\nbetween instruction-tuning datasets and the pre-training cor-"
}