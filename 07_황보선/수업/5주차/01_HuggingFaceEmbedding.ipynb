{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# 기존 PDF 파일 로드\n",
    "input_file = r\"C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\240120과제.pdf\"\n",
    "output_file = \"과제/241020/data.pdf\"  # 파일 확장자 포함\n",
    "\n",
    "# 디렉터리 생성\n",
    "output_dir = os.path.dirname(output_file)  # 디렉터리 경로 추출\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)  # 디렉터리가 없으면 생성\n",
    "\n",
    "# PDF 읽기 및 작성\n",
    "reader = PdfReader(input_file)\n",
    "writer = PdfWriter()\n",
    "\n",
    "# 여러 페이지 추가\n",
    "pages_to_extract = [0, 1, 2]\n",
    "for page_num in pages_to_extract:\n",
    "    writer.add_page(reader.pages[page_num])\n",
    "\n",
    "# 추출된 PDF 저장\n",
    "with open(output_file, \"wb\") as f:\n",
    "    writer.write(f)\n",
    "\n",
    "print(f\"페이지 {', '.join(str(p + 1) for p in pages_to_extract)}이 {output_file}로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = r\"C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\data.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_metadata(docs):\n",
    "    if docs:\n",
    "        print(\"[metadata]\")\n",
    "        print(list(docs[0].metadata.keys()))\n",
    "        print(\"\\n[examples]\")\n",
    "        max_key_length = max(len(k) for k in docs[0].metadata.keys())\n",
    "        for k, v in docs[0].metadata.items():\n",
    "            print(f\"{k:<{max_key_length}} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP\n",
      "Instruction Data\n",
      "Q: Why does MATLAB's pwelch function use 2k \n",
      "degrees of freedom for confidence intervals in \n",
      "power spectral density estimation, and what is \n",
      "the reasoning behind this choice?\n",
      "A: In the calculation of confidence intervals for \n",
      "power spectral density (PSD) estimation using \n",
      "Welch‘s method, MATLAB’s pwelch function \n",
      "uses 2k degrees of freedom instead of k-1 or 2k-\n",
      "1. This choice is based on ……\n",
      "Original SFT Dataset\n",
      "Combined Set\n",
      "Pretraining Corpus\n",
      "Difference Set\n",
      "Rewritten Set\n",
      "Rew\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# PyMuPDF 로더 인스턴스 생성\n",
    "loader = PyMuPDFLoader(FILE_PATH)\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 문서의 내용 출력\n",
    "print(docs[1].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[metadata]\n",
      "['source', 'file_path', 'page', 'total_pages', 'format', 'title', 'author', 'subject', 'keywords', 'creator', 'producer', 'creationDate', 'modDate', 'trapped']\n",
      "\n",
      "[examples]\n",
      "source       : C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\data.pdf\n",
      "file_path    : C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\data.pdf\n",
      "page         : 0\n",
      "total_pages  : 3\n",
      "format       : PDF 1.5\n",
      "title        : \n",
      "author       : \n",
      "subject      : \n",
      "keywords     : \n",
      "creator      : \n",
      "producer     : PyPDF2\n",
      "creationDate : \n",
      "modDate      : \n",
      "trapped      : \n"
     ]
    }
   ],
   "source": [
    "show_metadata(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=''\n",
    "os.environ['LANGCHAIN_TRACING_V2']=''\n",
    "os.environ['LANGCHAIN_ENDPOINT']=''\n",
    "os.environ['LANGCHAIN_API_KEY']=''\n",
    "os.environ['LANGCHAIN_PROJECT']=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "# 1. PDF Loading\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Load the text from a PDF file.\n",
    "    \n",
    "    :param file_path: Path to the PDF file.\n",
    "    :return: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Splitting the Text\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Split the text into chunks.\n",
    "    \n",
    "    :param text: The full text to split.\n",
    "    :param chunk_size: Maximum size of each chunk.\n",
    "    :param chunk_overlap: Overlap size between chunks.\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# 3. Embedding the Text\n",
    "def embed_text(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the text chunks using HuggingFaceEmbeddings.\n",
    "    \n",
    "    :param chunks: List of text chunks.\n",
    "    :return: A FAISS vectorstore containing the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings()  # Default model: all-MiniLM-L6-v2\n",
    "    vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# 4. Generate Questions Using GPT\n",
    "def generate_questions(text, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generate a list of questions based on the provided text using GPT.\n",
    "    \n",
    "    :param text: The input text for question generation.\n",
    "    :param num_questions: Number of questions to generate.\n",
    "    :return: List of questions.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(temperature=0.7)\n",
    "    prompt = f\"Generate {num_questions} questions based on the following text:\\n{text}\"\n",
    "    response = llm(prompt)\n",
    "    return response.split(\"\\n\")\n",
    "\n",
    "# 5. Find the Most Similar Embedding\n",
    "def find_most_similar(vectorstore, questions):\n",
    "    \"\"\"\n",
    "    Find the most similar embedding for each question.\n",
    "    \n",
    "    :param vectorstore: FAISS vectorstore with text embeddings.\n",
    "    :param questions: List of questions.\n",
    "    :return: Dictionary mapping questions to their most similar answers.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for question in questions:\n",
    "        docs = vectorstore.similarity_search(question, k=1)\n",
    "        results[question] = docs[0].page_content if docs else \"No matching content found.\"\n",
    "    return results\n",
    "\n",
    "# 6. Save Results to JSON\n",
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the data to a JSON file.\n",
    "    \n",
    "    :param data: Data to save.\n",
    "    :param file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_23556\\2636064405.py:48: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings()  # Default model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions and answers saved to C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\output.json\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "# 1. PDF Loading\n",
    "def load_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Load the text from a PDF file.\n",
    "    \n",
    "    :param file_path: Path to the PDF file.\n",
    "    :return: Extracted text from the PDF.\n",
    "    \"\"\"\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# 2. Splitting the Text\n",
    "def split_text(text, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    Split the text into chunks.\n",
    "    \n",
    "    :param text: The full text to split.\n",
    "    :param chunk_size: Maximum size of each chunk.\n",
    "    :param chunk_overlap: Overlap size between chunks.\n",
    "    :return: List of text chunks.\n",
    "    \"\"\"\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    return text_splitter.split_text(text)\n",
    "\n",
    "# 3. Embedding the Text\n",
    "def embed_text(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for the text chunks using HuggingFaceEmbeddings.\n",
    "    \n",
    "    :param chunks: List of text chunks.\n",
    "    :return: A FAISS vectorstore containing the embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings()  # Default model: all-MiniLM-L6-v2\n",
    "    vectorstore = FAISS.from_texts(chunks, embeddings)\n",
    "    return vectorstore\n",
    "\n",
    "# 4. Generate Questions Using GPT\n",
    "def generate_questions(text, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generate a list of questions based on the provided text using GPT.\n",
    "    \n",
    "    :param text: The input text for question generation.\n",
    "    :param num_questions: Number of questions to generate.\n",
    "    :return: List of questions.\n",
    "    \"\"\"\n",
    "    llm = OpenAI(temperature=0.7)\n",
    "    prompt = f\"Generate {num_questions} questions based on the following text:\\n{text}\"\n",
    "    response = llm(prompt)\n",
    "    return response.split(\"\\n\")\n",
    "\n",
    "# 5. Find the Most Similar Embedding\n",
    "def find_most_similar(vectorstore, questions):\n",
    "    \"\"\"\n",
    "    Find the most similar embedding for each question.\n",
    "    \n",
    "    :param vectorstore: FAISS vectorstore with text embeddings.\n",
    "    :param questions: List of questions.\n",
    "    :return: Dictionary mapping questions to their most similar answers.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for question in questions:\n",
    "        docs = vectorstore.similarity_search(question, k=1)\n",
    "        results[question] = docs[0].page_content if docs else \"No matching content found.\"\n",
    "    return results\n",
    "\n",
    "# 6. Save Results to JSON\n",
    "def save_to_json(data, file_path):\n",
    "    \"\"\"\n",
    "    Save the data to a JSON file.\n",
    "    \n",
    "    :param data: Data to save.\n",
    "    :param file_path: Path to the JSON file.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Main Workflow Example\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_path = r\"C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\data.pdf\"  # Replace with the actual file path\n",
    "    json_output_path = r\"C:\\Users\\user\\Desktop\\Github\\JJU1\\07_황보선\\수업\\5주차\\output.json\"  # Replace with the desired output path\n",
    "\n",
    "    # Step 1: Load the PDF\n",
    "    text = load_pdf(pdf_path)\n",
    "\n",
    "    # Step 2: Split the text\n",
    "    chunks = split_text(text)\n",
    "\n",
    "    # Step 3: Embed the text\n",
    "    vectorstore = embed_text(chunks)\n",
    "\n",
    "    # Step 4: Generate questions\n",
    "    questions = generate_questions(text, num_questions=10)\n",
    "\n",
    "    # Step 5: Find the most similar embeddings\n",
    "    results = find_most_similar(vectorstore, questions)\n",
    "\n",
    "    # Step 6: Save results to JSON\n",
    "    save_to_json(results, json_output_path)\n",
    "\n",
    "    print(f\"Questions and answers saved to {json_output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (PyTorch)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
