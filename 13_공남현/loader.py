# -*- coding: utf-8 -*-
"""Loader.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q2TpMiTod4u42bVabNWhWtXPagTnHOCA

## Sorter 1
"""

pip install -qU pypdf

pip install -qU langchain_community

pip install -qU langchain_teddynote

import os
from langchain_community.document_loaders import (
    PyPDFLoader,
    UnstructuredExcelLoader,
    CSVLoader,
    UnstructuredMarkdownLoader,
    WebBaseLoader,
)
from langchain_teddynote.document_loaders import HWPLoader

def sorter(file_path):
    """
    파일 형식에 따라 적합한 Loader를 선택하고 처리합니다.

    :param file_path: 처리할 파일 경로
    :return: 처리된 문서 리스트 (Document 객체)
    """
    # 확장자별 Loader 매핑
    loader_mapping = {
        ".pdf": PyPDFLoader,
        ".hwp": HWPLoader,
        ".csv": CSVLoader,
        ".xls": UnstructuredExcelLoader,
        ".xlsx": UnstructuredExcelLoader,
        ".txt": UnstructuredMarkdownLoader,
        ".json": UnstructuredMarkdownLoader,
        ".html": WebBaseLoader,
        ".md": UnstructuredMarkdownLoader,
    }

    # 파일 확장자 추출
    extension = os.path.splitext(file_path)[-1].lower()

    # 로더 선택
    loader_class = loader_mapping.get(extension)
    if not loader_class:
        raise ValueError(f"Unsupported file type: {extension}")

    # 선택된 Loader 호출
    loader = loader_class(file_path)
    docs = loader.load()

    return docs

# 테스트 파일 처리
file_path = "/content/data/sample.pdf"  # 예제 파일 경로
try:
    print(f"Processing file: {file_path}")
    docs = sorter(file_path)
    print(f"Loaded {len(docs)} documents.")
    print(f"First document content: {docs[0].page_content[:300]}")  # 첫 300자 출력
except Exception as e:
    print(f"Error: {e}")

"""# Loader

### PyPDF
"""

from langchain_community.document_loaders import PyPDFLoader

def pypdf_loader(file_path):
    """
    PyPDF를 사용하여 PDF 파일 처리.
    """
    loader = PyPDFLoader(file_path)
    docs = loader.load()
    return docs

"""### PyMuPDF"""

from langchain_community.document_loaders import PyMuPDFLoader

def pymupdf_loader(file_path):
    """
    PyMuPDF를 사용하여 PDF 파일 처리.
    """
    loader = PyMuPDFLoader(file_path)
    docs = loader.load()
    return docs

"""### PyPDFium2"""

from langchain_community.document_loaders import PyPDFium2Loader

def pypdfium2_loader(file_path):
    """
    PyPDFium2를 사용하여 PDF 파일 처리.
    """
    loader = PyPDFium2Loader(file_path)
    docs = loader.load()
    return docs

"""### PDFMinerPDFasHTML"""

from langchain_community.document_loaders import PDFMinerPDFasHTMLLoader

def pdfminer_loader(file_path):
    """
    PDFMiner를 사용하여 PDF 파일을 HTML로 처리.
    """
    loader = PDFMinerPDFasHTMLLoader(file_path)
    docs = loader.load()
    return docs

"""### PDFPlumber"""

from langchain_community.document_loaders import PDFPlumberLoader

def pdfplumber_loader(file_path):
    """
    PDFPlumber를 사용하여 PDF 파일 처리.
    """
    loader = PDFPlumberLoader(file_path)
    docs = loader.load()
    return docs

"""## hwp"""

from langchain_teddynote.document_loaders import HWPLoader

def hwp_loader(file_path):
    """
    HWP 파일을 처리하는 Loader.
    :param file_path: HWP 파일 경로
    :return: 처리된 문서 리스트
    """
    loader = HWPLoader(file_path)
    docs = loader.load()
    return docs

"""## CSV Loader"""

from langchain_community.document_loaders.csv_loader import CSVLoader

def csv_loader(file_path):
    """
    CSV 파일을 처리하는 Loader.
    :param file_path: CSV 파일 경로
    :return: 처리된 문서 리스트
    """
    loader = CSVLoader(file_path=file_path)
    docs = loader.load()
    return docs

"""

> ## Excel Loader

"""

from langchain_community.document_loaders import UnstructuredExcelLoader

def excel_loader(file_path):
    """
    Excel 파일을 처리하는 Loader.
    :param file_path: Excel 파일 경로
    :return: 처리된 문서 리스트
    """
    loader = UnstructuredExcelLoader(file_path=file_path)
    docs = loader.load()
    return docs

"""## TXT Loader"""

from langchain_community.document_loaders import UnstructuredMarkdownLoader

def txt_loader(file_path):
    """
    TXT 파일을 처리하는 Loader.
    :param file_path: TXT 파일 경로
    :return: 처리된 문서 리스트
    """
    loader = UnstructuredMarkdownLoader(file_path)
    docs = loader.load()
    return docs

"""## JSON Loader"""

from langchain_community.document_loaders import JSONLoader

def json_loader(file_path):
    """
    JSON 파일을 처리하는 Loader.
    :param file_path: JSON 파일 경로
    :return: 처리된 문서 리스트
    """
    loader = JSONLoader(file_path=file_path, jq_schema=".")
    docs = loader.load()
    return docs

"""## WebBase Loader"""

from langchain_community.document_loaders import WebBaseLoader
import bs4

def webbase_loader_html(file_path):
    """
    HTML 파일을 WebBaseLoader로 처리하는 함수.
    :param file_path: HTML 파일 경로
    :return: 처리된 문서 리스트
    """
    # HTML 파일 읽기
    with open(file_path, "r", encoding="utf-8") as file:
        html_content = file.read()

    # WebBaseLoader로 처리
    loader = WebBaseLoader(
        web_paths=(html_content,),  # HTML 파일 내용을 WebBaseLoader로 전달
        bs_kwargs=dict(
            parse_only=bs4.SoupStrainer(
                "div",
                attrs={"class": ["newsct_article _article_body", "media_end_head_title"]},
            )
        ),
        header_template={
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36",
        },
    )
    docs = loader.load()
    return docs

"""# Sorter 2"""

from langchain.text_splitter import (
    CharacterTextSplitter,
    RecursiveCharacterTextSplitter,
    TokenTextSplitter,
    MarkdownHeaderTextSplitter,
    HTMLHeaderTextSplitter,
)

def sorter_II(docs, file_type):
    """
    Loader에서 처리된 데이터를 적합한 Splitter로 전달하는 함수.

    :param docs: Loader가 반환한 문서 리스트
    :param file_type: 파일 형식 (ex: 'pdf', 'csv', 'txt', 'html', 'json')
    :return: 분할된 텍스트 조각 리스트
    """
    # Splitter 매핑
    splitter_mapping = {
        "pdf": CharacterTextSplitter(chunk_size=1000, chunk_overlap=100),
        "txt": RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100),
        "csv": TokenTextSplitter(chunk_size=200, chunk_overlap=50),
        "excel": TokenTextSplitter(chunk_size=200, chunk_overlap=50),
        "json": RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100),
        "html": HTMLHeaderTextSplitter(),
        "md": MarkdownHeaderTextSplitter(),
    }

    # 적합한 Splitter 선택
    splitter = splitter_mapping.get(file_type)
    if not splitter:
        raise ValueError(f"Unsupported file type for splitting: {file_type}")

    # Splitter를 사용하여 텍스트 분할
    all_chunks = []
    for doc in docs:
        chunks = splitter.split_text(doc.page_content)
        all_chunks.extend(chunks)

    return all_chunks

"""# Splitter

###  CharacterTextSplitter
"""

from langchain.text_splitter import CharacterTextSplitter

def character_text_splitter(chunk_size=1000, chunk_overlap=100):
    """
    CharacterTextSplitter를 생성하는 함수.
    :param chunk_size: 하나의 텍스트 조각 크기
    :param chunk_overlap: 조각 간 중첩 크기
    :return: CharacterTextSplitter 객체
    """
    return CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

"""### RecursiveCharacterTextSplitter"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

def recursive_character_text_splitter(chunk_size=1000, chunk_overlap=100):
    """
    RecursiveCharacterTextSplitter를 생성하는 함수.
    :param chunk_size: 하나의 텍스트 조각 크기
    :param chunk_overlap: 조각 간 중첩 크기
    :return: RecursiveCharacterTextSplitter 객체
    """
    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

"""### TokenTextSplitter"""

from langchain.text_splitter import TokenTextSplitter

def token_text_splitter(chunk_size=200, chunk_overlap=50):
    """
    TokenTextSplitter를 생성하는 함수.
    :param chunk_size: 하나의 텍스트 조각 크기 (토큰 단위)
    :param chunk_overlap: 조각 간 중첩 크기
    :return: TokenTextSplitter 객체
    """
    return TokenTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

"""### SemanticChunker"""

def semantic_chunker(text, chunk_size=1000):
    """
    텍스트를 의미론적으로 분할하는 커스텀 함수.
    :param text: 입력 텍스트
    :param chunk_size: 의미론적 분할 크기
    :return: 분할된 텍스트 리스트
    """
    chunks = []
    while text:
        chunk = text[:chunk_size]
        chunks.append(chunk.strip())
        text = text[chunk_size:]
    return chunks

"""### CodeSplitter"""

def code_splitter(code, delimiter="\n"):
    """
    코드 파일을 특정 구분자(기본적으로 줄 단위)로 분할하는 함수.
    :param code: 입력 코드 텍스트
    :param delimiter: 코드 구분자
    :return: 분할된 코드 조각 리스트
    """
    return code.split(delimiter)

"""### MarkdownHeaderTextSplitter"""



from langchain.text_splitter import MarkdownHeaderTextSplitter
from langchain_community.document_loaders import UnstructuredMarkdownLoader

def markdown_splitter(file_path):
    """
    Markdown 파일을 처리하고 MarkdownHeaderTextSplitter로 분할하는 함수.
    :param file_path: Markdown 파일 경로
    :return: 분할된 텍스트 조각 리스트
    """
    # 1. Markdown 파일 로드
    loader = UnstructuredMarkdownLoader(file_path)
    docs = loader.load()  # Loader가 반환한 문서 리스트

    # 2. MarkdownHeaderTextSplitter 생성
    splitter = MarkdownHeaderTextSplitter()

    # 3. Splitter를 사용하여 텍스트 분할
    chunks = []
    for doc in docs:
        chunks.extend(splitter.split_text(doc.page_content))  # 각 문서를 분할하여 chunks 리스트에 추가

    return chunks

"""### HTMLHeaderTextSplitter"""

def markdown_header_splitter():
    """
    Markdown 문서를 MarkdownHeaderTextSplitter로 분할.
    :return: MarkdownHeaderTextSplitter 객체
    """
    # 헤더 정의
    headers_to_split_on = [
        ("#", "H1"),  # 헤더 레벨 1
        ("##", "H2"), # 헤더 레벨 2
        ("###", "H3"), # 헤더 레벨 3
    ]

    return MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)

from langchain.text_splitter import MarkdownHeaderTextSplitter

def markdown_header_splitter():
    """
    Markdown 문서를 MarkdownHeaderTextSplitter로 분할.
    :return: MarkdownHeaderTextSplitter 객체
    """
    return MarkdownHeaderTextSplitter()

from langchain.text_splitter import HTMLHeaderTextSplitter

def html_header_text_splitter():
    """
    HTMLHeaderTextSplitter를 생성하는 함수.
    :return: HTMLHeaderTextSplitter 객체
    """
    return HTMLHeaderTextSplitter()

"""### RecursiveJsonSplitter"""

def recursive_json_splitter(json_data, prefix=""):
    """
    JSON 데이터를 재귀적으로 분할하는 함수.
    :param json_data: JSON 데이터
    :param prefix: 키 경로를 나타내는 문자열
    :return: 분할된 JSON 데이터 리스트
    """
    chunks = []
    if isinstance(json_data, dict):
        for key, value in json_data.items():
            chunks.extend(recursive_json_splitter(value, f"{prefix}.{key}" if prefix else key))
    elif isinstance(json_data, list):
        for i, item in enumerate(json_data):
            chunks.extend(recursive_json_splitter(item, f"{prefix}[{i}]"))
    else:
        chunks.append(f"{prefix}: {json_data}")
    return chunks

"""# OpenAI"""

def evaluate_loader_splitter(file_path, loader_function, splitter_function, llm="openai", **llm_kwargs):
    """
    Loader와 Splitter 조합을 테스트하여 LLM 성능 평가.
    :param file_path: 입력 파일 경로
    :param loader_function: 사용할 Loader 함수
    :param splitter_function: 사용할 Splitter 함수
    :param llm: 사용할 LLM ("openai" 또는 "gemini")
    :param llm_kwargs: LLM에 전달할 추가 매개변수
    :return: LLM 응답 리스트
    """
    try:
        # 1. Loader 실행
        docs = loader_function(file_path)

        # 2. Splitter 실행
        chunks = splitter_function(docs)

        # 3. LLM 호출
        responses = process_with_llm(chunks, llm=llm, **llm_kwargs)

        return responses

    except Exception as e:
        print(f"Error in evaluate_loader_splitter: {e}")
        return None

def evaluate_all(file_path, file_type, llm="openai", **llm_kwargs):
    """
    모든 Loader와 Splitter 조합을 테스트하여 성능 평가.
    :param file_path: 입력 파일 경로
    :param file_type: 파일 형식 (ex: "pdf", "csv", "txt")
    :param llm: 사용할 LLM ("openai" 또는 "gemini")
    :param llm_kwargs: LLM에 전달할 추가 매개변수
    :return: 성능 평가 결과 딕셔너리
    """
    results = {}

    # 1. 해당 파일 형식에 적합한 Loader 선택
    loader_function = loaders.get(file_type)
    if not loader_function:
        print(f"Unsupported file type: {file_type}")
        return None

    # 2. 모든 Splitter와 조합 테스트
    for splitter_name, splitter_function in splitters.items():
        print(f"\nTesting with Splitter: {splitter_name}")
        responses = evaluate_loader_splitter(
            file_path=file_path,
            loader_function=loader_function,
            splitter_function=splitter_function(),
            llm=llm,
            **llm_kwargs
        )
        if responses:
            # LLM 응답 기록
            results[splitter_name] = responses

    return results

# Loader 함수 정의
loaders = {
    "pdf": pdf_loader,
    "hwp": hwp_loader,
    "csv": csv_loader,
    "excel": excel_loader,
    "txt": txt_loader,
    "json": json_loader,
    "html": webbase_loader_html,
    "md": markdown_splitter,
}

# Splitter 함수 정의
splitters = {
    "CharacterTextSplitter": character_text_splitter,
    "RecursiveCharacterTextSplitter": recursive_character_text_splitter,
    "TokenTextSplitter": token_text_splitter,
    "MarkdownHeaderTextSplitter": markdown_header_splitter,
    "HTMLHeaderTextSplitter": html_header_text_splitter,
}

import openai

def process_with_llm(chunks, llm="openai", **kwargs):
    """
    다양한 LLM을 사용하여 텍스트 조각 처리.
    :param chunks: 텍스트 조각 리스트
    :param llm: 사용할 LLM ("openai" 또는 "gemini")
    :param kwargs: LLM에 전달할 추가 매개변수
    :return: 처리된 결과 리스트
    """
    results = []

    for chunk in chunks:
        try:
            if llm == "openai":
                # OpenAI API 호출
                response = openai.ChatCompletion.create(
                    model=kwargs.get("model", "gpt-4"),
                    messages=[{"role": "user", "content": chunk}],
                    temperature=kwargs.get("temperature", 0.7),
                )
                results.append(response["choices"][0]["message"]["content"])
            elif llm == "gemini":
                # Gemini API 호출 (임시 구현)
                response = {"generated_text": f"Gemini processed: {chunk[:30]}..."}
                results.append(response["generated_text"])
            else:
                raise ValueError(f"Unsupported LLM: {llm}")

        except Exception as e:
            print(f"Error processing chunk: {e}")
            results.append(None)

    return results

def print_results(results):
    """
    성능 평가 결과 출력.
    :param results: 성능 평가 결과 딕셔너리
    """
    print("\n=== Evaluation Results ===")
    for splitter_name, responses in results.items():
        print(f"\nSplitter: {splitter_name}")
        for i, response in enumerate(responses):
            print(f"Chunk {i + 1}: {response}")

# 파일 경로 및 형식 설정
file_path = "/content/data/sample.md"
file_type = "md"

# OpenAI API를 사용하여 평가
results = evaluate_all(
    file_path=file_path,
    file_type=file_type,
    llm="openai",
    model="gpt-4",
    temperature=0.7,
)

# 결과 출력
print_results(results)