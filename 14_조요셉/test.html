<h1 id='0' style='font-size:22px'>Don't Do RAG:<br>When Cache-Augmented Generation is All You Need for<br>Knowledge Tasks</h1>
<br><p id='2' data-category='paragraph' style='font-size:20px'>Brian J Chan*<br>Chao-Ting Chen*<br>Jui-Hung Cheng*<br>Department of Computer Science<br>National Chengchi University<br>Taipei, Taiwan<br>{110703065, 110703038.110708007/@nccu.edu.tw</p>
<br><p id='3' data-category='paragraph' style='font-size:18px'>Hen-Hsen Huang<br>Insititue of Information Science<br>Academia Sinica<br>Taipei, Taiwan<br>hhhuang@iis.sinica.edu.tw</p>
<p id='4' data-category='paragraph' style='font-size:20px'>Abstract</p>
<br><p id='5' data-category='paragraph' style='font-size:16px'>Retrieval-augmented generation (RAG) has gained traction as a<br>powerful approach for enhancing language models by integrating<br>external knowledge sources. However, RAG introduces challenges<br>such as retrieval latency, potential errors in document selection,<br>and increased system complexity. With the advent of large lan-<br>guage models (LLMs) featuring significantly extended context win-<br>dows, this paper proposes an alternative paradigm, cache-augmented<br>generation (CAG) that bypasses real-time retrieval. Our method in-<br>volves preloading all relevant resources, especially when the docu-<br>ments or knowledge for retrieval are of a limited and manageable<br>size, into the LLM's extended context and caching its runtime pa-<br>rameters. During inference, the model utilizes these preloaded pa-<br>rameters to answer queries without additional retrieval steps. Com-<br>parative analyses reveal that CAG eliminates retrieval latency and<br>minimizes retrieval errors while maintaining context relevance. Per-<br>formance evaluations across multiple benchmarks highlight sce-<br>narios where long-context LLMs either outperform or complement<br>traditional RAG pipelines. These findings suggest that, for certain<br>applications, particularly those with a constrained knowledge base,<br>CAG provide a streamlined and efficient alternative to RAG, achiev-<br>ing comparable or superior results with reduced complexity.</p>
<p id='6' data-category='paragraph' style='font-size:20px'>CCS Concepts</p>
<br><p id='7' data-category='paragraph' style='font-size:16px'>· Computing methodologies → Discourse, dialogue and prag-<br>matics; Natural language generation; · Information systems<br>→ Specialized information retrieval.</p>
<br><p id='8' data-category='paragraph' style='font-size:22px'>Keywords</p>
<br><p id='9' data-category='paragraph' style='font-size:18px'>Large Language Models, Retrieval Augmented Generation, Retrieval-<br>Free Question Answering</p>
<p id='10' data-category='paragraph' style='font-size:18px'>1 Introduction</p>
<br><p id='11' data-category='paragraph' style='font-size:16px'>The advent of retrieval-augmented generation (RAG) [1, 3] has<br>significantly enhanced the capabilities of large language models<br>(LLMs) by dynamically integrating external knowledge sources. RAG<br>systems have proven effective in handling open-domain questions<br>and specialized tasks, leveraging retrieval pipelines to provide con-<br>textually relevant answers. However, RAG is not without its draw-<br>backs. The need for real-time retrieval introduces latency, while</p>
<p id='12' data-category='footnote' style='font-size:16px'>`Three authors contributed equally to this research.</p>
<br><figure><img id='13' style='font-size:14px' alt="LLM
Retrieval
Model A2
LLM
Knowledge
A1 A2
Pre-compute
LLM LLM LLM
Append Q1 Truncate Q1
Knowledge Knowledge Knowledge
Q1 Append Q2 Q2
Cache Cache Cache
Q1 2 02" data-coord="top-left:(661,501); bottom-right:(1163,846)" /></figure>
<caption id='14' style='font-size:16px'>Figure 1: Comparison of Traditional RAG and our CAG<br>Workflows: The upper section illustrates the RAG pipeline,<br>including real-time retrieval and reference text input dur-<br>ing inference, while the lower section depicts our CAG ap-<br>proach, which preloads the KV-cache, eliminating the re-<br>trieval step and reference text input at inference.</caption>
<p id='15' data-category='paragraph' style='font-size:16px'>errors in selecting or ranking relevant documents can degrade the<br>quality of the generated responses. Additionally, integrating re-<br>trieval and generation components increases system complexity,<br>necessitating careful tuning and adding to the maintenance over-<br>head.</p>
<br><p id='16' data-category='paragraph' style='font-size:16px'>This paper proposes an alternative paradigm, cache-augmented<br>generation (CAG), leveraging the capabilities oflong-context LLMs<br>to address these challenges. Instead ofrelying on a retrieval pipeline,<br>as shown in Figure 1, our approach involves preloading the LLM<br>with all relevant documents in advance and precomputing the key-<br>value (KV) cache, which encapsulates the inference state of the<br>LLM. The preloaded context enables the model to provide rich,<br>contextually accurate answers without the need for additional re-<br>trieval during runtime. This approach eliminates retrieval latency,<br>mitigates retrieval errors, and simplifies system architecture, all<br>while maintaining high-quality responses by ensuring the model<br>processes all relevant context holistically.</p>
<p id='19' data-category='paragraph' style='font-size:16px'>Recent advances in long-context LLMs have extended their abil-<br>ity to process and reason over substantial textual inputs. By ac-<br>commodating larger context windows, these models can assimi-<br>late extensive information in a single inference step, making them<br>well-suited for tasks like document comprehension, multi-turn di-<br>alogue, and summarization of lengthy texts. This capability elimi-<br>nates the dependency on real-time retrieval, as all necessary infor-<br>mation can be preloaded into the model. These developments cre-<br>ate opportunities to streamline workflows for knowledge-intensive<br>tasks, potentially reducing or even eliminating the need for tradi-<br>tional RAG systems.</p>
<br><p id='20' data-category='paragraph' style='font-size:16px'>Recent studies [2, 4] have investigated the performance of long-<br>context models in RAG tasks, revealing that state-of-the-art mod-<br>els like GPT-o1, GPT-4, and Claude 3.5 can effectively process large<br>amounts of retrieved data, outperforming traditional systems in<br>many scenarios. Findings suggest that as long as all documents<br>fit within the extended context length, traditional RAG systems<br>can be replaced by these long-context models. Similarly, Lu et al.<br>[5] has demonstrated the benefits of precomputed KV caching to<br>improve efficiency, albeit with the need for position ID rearrange-<br>ment to enable proper functioning. Nonetheless, these methods re-<br>main vulnerable to retrieval failures inherent to RAG systems.</p>
<br><p id='21' data-category='paragraph' style='font-size:16px'>Through a series of experiments comparing traditional RAG work-<br>flows with our proposed approach, we identify scenarios where<br>long-context LLMs outperform RAG in both efficiency and accu-<br>racy. By addressing the technical and practical implications, this<br>paper aims to provide insights into when and why CAG may serve<br>as a streamlined, effective alternative to RAG, particularly for cases<br>where the documents or knowledge for retrieval are of limited,<br>manageable size. Our findings challenge the default reliance on<br>RAG for knowledge integration tasks, offering a simplified, robust<br>solution to harness the growing capabilities of long-context LLMs.<br>Our contributions are threefold as follows:</p>
<p id='22' data-category='list' style='font-size:16px'>· Retrieval-Free Long-Context Paradigm: Introduced a novel<br>approach leveraging long-context LLMs with preloaded doc-<br>uments and precomputed KV caches, eliminating retrieval<br>latency, errors, and system complexity.<br>· Performance Comparison: Conducted extensive experiments<br>showing scenarios where long-context LLMs outperform tra-<br>ditional RAG systems, especially with manageable knowl-<br>edge bases.<br>· Practical Insights: Provided actionable insights into optimiz-<br>ing knowledge-intensive workflows, demonstrating the via-<br>bility of retrieval-free methods for specific applications. Our<br>CAG framework is released publicly.1</p>
<h1 id='23' style='font-size:20px'>2 Methodology</h1>
<p id='24' data-category='paragraph' style='font-size:16px'>Our CAG framework leverages the extended context capabilities of<br>long-context LLMs to enable retrieval-free knowledge integration.<br>By preloading external knowledge sources, such as a collection<br>of documents D = {d1, d2, · · · }, and precomputing the key-value<br>(KV) cache CKV, we address the computational challenges and in-<br>efficiencies inherent to real-time retrieval in traditional RAG sys-<br>tems. The operation of our framework is divided into three phases:</p>
<p id='25' data-category='footnote' style='font-size:20px'>1https://github.com/nhhuang/CAG</p>
<br><p id='26' data-category='paragraph' style='font-size:20px'>(1) External Knowledge Preloading</p>
<br><p id='27' data-category='paragraph' style='font-size:16px'>In this phase, a curated collection of documents D relevant<br>to the target application is preprocessed and formatted to<br>fit within the model's extended context window. The LLM<br>M, with parameters 0, processes D, transforming it into a<br>precomputed KV cache:</p>
<p id='28' data-category='paragraph' style='font-size:18px'>CKV = KV-Encode(D) (1)</p>
<br><p id='29' data-category='paragraph' style='font-size:16px'>This KV cache, which encapsulates the inference state of<br>the LLM, is stored on disk or in memory for future use. The<br>computational cost of processing D is incurred only once,<br>regardless of the number of subsequent queries.</p>
<br><p id='30' data-category='paragraph' style='font-size:20px'>(2) Inference</p>
<br><p id='31' data-category='paragraph' style='font-size:16px'>During inference, the precomputed KV cache CKV is loaded<br>alongside the user's query Q. The LLM utilizes this cached<br>context to generate responses:</p>
<br><p id='32' data-category='equation'>$${\mathcal{R}}={\mathcal{M}}(Q\mid C_{\mathrm{KV}})$$</p>
<br><caption id='33' style='font-size:22px'>(2)</caption>
<p id='34' data-category='paragraph' style='font-size:16px'>By preloading the external knowledge, this phase eliminates<br>retrieval latency and reduces risks of errors or omissions<br>that arise from dynamic retrieval. The combined prompt<br>P = Concat (D, Q) ensures a unified understanding ofboth<br>the external knowledge and the user query.</p>
<br><p id='35' data-category='paragraph' style='font-size:18px'>(3) Cache Reset</p>
<br><p id='36' data-category='paragraph' style='font-size:14px'>To maintain system performance across multiple inference<br>sessions, the KV cache, stored in memory, can be reset effi-<br>ciently. As the KV cache grows in an append-only manner<br>with new tokens t1, t2, · · · , tk sequentially appended, reset-<br>ting involves truncating these new tokens:</p>
<p id='37' data-category='equation'>$$C_{\mathrm{KV}}^{\mathrm{resct}}=\mp r\mathrm{tr}\mathrm{te}\left(G_{\mathrm{KV}},t_{1},t_{2},\dots,t_{k}\right)\qquad\qquad\qquad(3)$$</p>
<br><p id='38' data-category='paragraph' style='font-size:16px'>This allows for rapid reinitialization without reloading the<br>entire cache from disk, ensuring sustained speed and re-<br>sponsiveness.</p>
<br><p id='39' data-category='paragraph' style='font-size:16px'>The proposed methodology offers several significant advantages<br>over traditional RAG systems:</p>
<br><p id='40' data-category='list' style='font-size:16px'>· Reduced Inference Time: By eliminating the need for real-<br>time retrieval, the inference process becomes faster and more<br>efficient, enabling quicker responses to user queries.<br>· Unified Context: Preloading the entire knowledge collec-<br>tion into the LLM provides a holistic and coherent under-<br>standing of the documents, resulting in improved response<br>quality and consistency across a wide range of tasks.<br>· Simplified Architecture: By removing the need to inte-<br>grate retrievers and generators, the system becomes more<br>streamlined, reducing complexity, improving maintainabil-<br>ity, and lowering development overhead.</p>
<br><p id='41' data-category='paragraph' style='font-size:16px'>Looking forward, our approach is poised to become even more<br>powerful with the anticipated advancements in LLMs. As future<br>models continue to expand their context length, they will be able<br>to process increasingly larger knowledge collections in a single in-<br>ference step. Additionally, the improved ability of these models to<br>extract and utilize relevant information from long contexts will<br>further enhance their performance. These two trends will signifi-<br>cantly extend the usability of our approach, enabling it to handle<br>more complex and diverse applications. Consequently, our method-<br>ology is well-positioned to become a robust and versatile solution</p>
<p id='42' data-category='paragraph' style='font-size:14px'>Don't Do RAG:<br>When Cache-Augmented Generation is All You Need for Knowledge Tasks</p>
<p id='44' data-category='paragraph' style='font-size:18px'>for knowledge-intensive tasks, leveraging the growing capabilities<br>of next-generation LLMs.</p>
<table id='45' style='font-size:16px'><tr><td>Source</td><td>Size</td><td># Docs</td><td># Tokens</td><td># QA Pairs</td></tr><tr><td rowspan="3">HotPotQA</td><td>Small</td><td>16</td><td>21k</td><td>1,392</td></tr><tr><td>Medium</td><td>32</td><td>43k</td><td>1,056</td></tr><tr><td>Large</td><td>64</td><td>85k</td><td>1,344</td></tr><tr><td rowspan="3">SQuAD</td><td>Small</td><td>3</td><td>21k</td><td>500</td></tr><tr><td>Medium</td><td>4</td><td>32k</td><td>500</td></tr><tr><td>Large</td><td>7</td><td>50k</td><td>500</td></tr></table>
<br><p id='46' data-category='paragraph' style='font-size:16px'>Table 1: Overview of the SQuAD and HotPotQA test sets<br>with varying reference text lengths, highlighting the num-<br>ber of documents, questions, and associated responses for<br>each configuration.</p>
<h1 id='47' style='font-size:20px'>3 Experiments</h1>
<br><p id='48' data-category='paragraph' style='font-size:22px'>3.1 Experimental Setup</p>
<p id='49' data-category='paragraph' style='font-size:16px'>To evaluate the effectiveness of our proposed method, we conducted<br>experiments using two widely recognized question-answering bench-<br>marks: the Stanford Question Answering Dataset (SQuAD) 1.0 [6]<br>and the HotPotQA dataset [7]. These datasets provide complemen-<br>tary challenges, with SQuAD focusing on precise, context-aware<br>answers within single passages and HotPotQA emphasizing multi-<br>hop reasoning across multiple documents. Each of both datasets<br>consists of documents D = {d1, d2, · · · } paired with questions<br>Qs = {q1, 92, · · · } and golden responses R = {r1, r2, · · · }. These<br>datasets provide a robust platform for assessing both single-context<br>comprehension and complex multi-hop reasoning.</p>
<br><p id='50' data-category='paragraph' style='font-size:16px'>To investigate how different levels of reference text length im-<br>pact retrieval difficulty, we created three test sets for each dataset,<br>varying the size of the reference text. For example, in the HotPotQA-<br>small configuration, we sampled 16 documents Ds C D from the<br>HotPotQA document set to form a long reference text. QA pairs as-<br>sociated with Ds were selected as test instances. The same method-<br>ology was applied to create test sets for SQuAD.</p>
<br><p id='51' data-category='paragraph' style='font-size:16px'>The dataset statistics are summarized in Table 1. As the number<br>of documents (and hence the length of the reference text) increases,<br>the task becomes more challenging, particularly for RAG systems.<br>Longer reference texts increase the difficulty of accurately retriev-<br>ing the correct information, which is crucial for LLMs to generate<br>high-quality responses.</p>
<br><p id='52' data-category='paragraph' style='font-size:16px'>The primary task involves generating accurate and contextually<br>relevant answers R = {f1, r2, . · · } for the SQuAD and HotPotQA<br>questions, based on the respective preloaded passages. By leverag-<br>ing the precomputed key-value cache CKV = KV-Encode (D), our<br>system generates responses Pi = M(qi I CKV) without relying on<br>retrieval mechanisms during inference. This unified approach al-<br>lows for direct performance comparisons against traditional RAG<br>systems, highlighting the strengths and limitations of our method<br>across diverse QA challenges.</p>
<br><p id='53' data-category='paragraph' style='font-size:16px'>The experiments were executed on Tesla V100 32G x 8 GPUs.<br>For all experiments, we used the Llama 3.1 8B Instruction model<br>as the underlying LLM across all systems, including both the RAG</p>
<br><p id='54' data-category='paragraph' style='font-size:16px'>baselines and our proposed method. This model supports input<br>sizes of up to 128k tokens, enabling the processing of extensive<br>contexts. For our proposed method, the context of each dataset<br>was preloaded into the model via a precomputed key-value (KV)<br>cache. For SQuAD, the documents Ds were encoded into a KV<br>cache CKV = KV-Encode (Ds), while for HotPotQA, the documents<br>DH were encoded into CKV = KV-Encode (DH). These caches were<br>stored offline and loaded during inference to eliminate the need for<br>real-time retrieval, ensuring comprehensive access to all relevant<br>information for each dataset.</p>
<p id='55' data-category='paragraph' style='font-size:20px'>3.2 Baseline Systems</p>
<br><p id='56' data-category='paragraph' style='font-size:16px'>The baseline RAG systems were implemented using the LlamaIn-<br>dex framework, 2 employing two retrieval strategies: BM25 for sparse<br>retrieval and OpenAI Indexes for dense retrieval. Each dataset-SQuAD<br>and HotPotQA-was evaluated separately, with retrieval systems<br>configured to fetch passages exclusively from the respective dataset<br>to ensure focused and fair evaluation. The details of each baseline<br>system are as follows:</p>
<p id='57' data-category='paragraph' style='font-size:16px'>(1) Sparse Retrieval System (BM25): The first baseline sys-<br>tem employed BM25 indexes for retrieval. BM25, a sparse re-<br>trieval algorithm, ranks documents based on term frequency-<br>inverse document frequency (TF-IDF) and document length<br>normalization. Given a query qi, BM25 retrieves the top-k<br>passages Pk = {P1, P2, · · · , Pk} from the indexed collection<br>D. These passages were then passed to the generator, M,<br>to synthesize answers:</p>
<br><p id='58' data-category='equation'>$$\dot{\tau}_{i}=\mathcal{N}(q_{i}\mid\mathcal{P}_{k})$$</p>
<br><caption id='59' style='font-size:22px'>(4)</caption>
<p id='60' data-category='list' style='font-size:16px'>BM25 provides a robust and interpretable retrieval mecha-<br>nism, suited for tasks involving keyword matching.<br>(2) Dense Retrieval System (OpenAI Indexes) The second<br>baseline utilized OpenAI indexes, 3 which employ dense em-<br>beddings to represent both documents and queries in a shared<br>semantic space. For a query qi, dense retrieval selects the<br>top-k passages Pk that semantically align with the query,<br>offering improved contextual understanding compared to<br>sparse methods. These passages were similarly passed to<br>the generator for answer synthesis as Equation 4. This sys-<br>tem is particularly effective for questions requiring nuanced<br>contextual matching beyond exact term overlap.</p>
<br><p id='61' data-category='paragraph' style='font-size:16px'>Our experiments were conducted on both the SQuAD and Hot-<br>PotQA datasets to evaluate the performance of different systems<br>in terms of similarity to ground-truth answers, measured using<br>BERTScore [8]. For the RAG baselines, the top-1, top-3, top-5, and<br>top-10 retrieved passages were used for inference. In contrast, our<br>CAG utilized the preloaded context specific to each dataset to gen-<br>erate answers without retrieval constraints.</p>
<p id='62' data-category='paragraph' style='font-size:18px'>3.3 Results</p>
<br><p id='63' data-category='paragraph' style='font-size:16px'>As shown in Table 2, the experimental results revealed clear distinc-<br>tions between our proposed method and traditional RAG systems.<br>Our proposed approach achieved the highest BERTScore in most</p>
<p id='64' data-category='footnote' style='font-size:18px'>Phttps//www.Ilamaindex.ai/framework<br>Prtps://ccklook.penai.com/inple/infordionaxing. with_llamaindex</p>
<caption id='67' style='font-size:18px'>Table 2: Experimental Results</caption>
<table id='68' style='font-size:14px'><tr><td>Size</td><td>System</td><td>Top-k</td><td>HotPotQA BERT-Score</td><td>SQuAD BERT-Score</td></tr><tr><td rowspan="9">Small</td><td rowspan="4">Sparse RAG</td><td>1</td><td>0.0673</td><td>0.7469</td></tr><tr><td>3</td><td>0.0673</td><td>0.7999</td></tr><tr><td>5</td><td>0.7549</td><td>0.8022</td></tr><tr><td>10</td><td>0.7461</td><td>0.8191</td></tr><tr><td rowspan="4">Dense RAG</td><td>1</td><td>0.7079</td><td>0.6445</td></tr><tr><td>3</td><td>0.7509</td><td>0.7304</td></tr><tr><td>5</td><td>0.7414</td><td>0.7583</td></tr><tr><td>10</td><td>0.7516</td><td>0.8035</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7759</td><td>0.8265</td></tr><tr><td rowspan="9">Medium</td><td rowspan="4">Sparse RAG</td><td>1</td><td>0.6652</td><td>0.7036</td></tr><tr><td>3</td><td>0.7619</td><td>0.7471</td></tr><tr><td>5</td><td>0.7616</td><td>0.7467</td></tr><tr><td>10</td><td>0.7238</td><td>0.7420</td></tr><tr><td rowspan="4">Dense RAG</td><td>1</td><td>0.7135</td><td>0.6188</td></tr><tr><td>3</td><td>0.7464</td><td>0.6869</td></tr><tr><td>5</td><td>0.7278</td><td>0.7047</td></tr><tr><td>10</td><td>0.7451</td><td>0.7350</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7696</td><td>0.7512</td></tr><tr><td rowspan="9">Large</td><td rowspan="4">Sparse RAG</td><td>1</td><td>0.6567</td><td>0.7135</td></tr><tr><td>3</td><td>0.7424</td><td>0.7510</td></tr><tr><td>5</td><td>0.7495</td><td>0.7543</td></tr><tr><td>10</td><td>0.7358</td><td>0.7548</td></tr><tr><td rowspan="4">Dense RAG</td><td>1</td><td>0.6969</td><td>0.6057</td></tr><tr><td>3</td><td>0.7426</td><td>0.6908</td></tr><tr><td>5</td><td>0.7300</td><td>0.7169</td></tr><tr><td>10</td><td>0.7398</td><td>0.7499</td></tr><tr><td>CAG (Ours)</td><td></td><td>0.7527</td><td>0.7640</td></tr></table>
<caption id='69' style='font-size:18px'>Table 3: Comparison of Generation Time</caption>
<table id='70' style='font-size:16px'><tr><td>Dataset</td><td>Size</td><td>System</td><td>Generation Time (s)</td></tr><tr><td rowspan="6">HotpotQA</td><td rowspan="2">Small</td><td>CAG</td><td>0.85292</td></tr><tr><td>w/o CAG</td><td>9.24734</td></tr><tr><td rowspan="2">Medium</td><td>CAG</td><td>1.66132</td></tr><tr><td>w/o CAG</td><td>28.81642</td></tr><tr><td rowspan="2">Large</td><td>CAG</td><td>2.32667</td></tr><tr><td>w/o CAG</td><td>94.34917</td></tr><tr><td rowspan="6">SQuAD</td><td rowspan="2">Small</td><td>CAG</td><td>1.06509</td></tr><tr><td>w/o CAG</td><td>10.29533</td></tr><tr><td rowspan="2">Medium</td><td>CAG</td><td>1.73114</td></tr><tr><td>w/o CAG</td><td>13.35784</td></tr><tr><td rowspan="2">Large</td><td>CAG</td><td>2.40577</td></tr><tr><td>w/o CAG</td><td>31.08368</td></tr></table>
<p id='71' data-category='paragraph' style='font-size:16px'>situations, outperforming both RAG systems. By preloading the en-<br>tire context from the test set, our system eliminates retrieval errors<br>and ensures holistic reasoning over all relevant information. This<br>advantage is particularly evident in scenarios where RAG systems</p>
<br><p id='72' data-category='paragraph' style='font-size:16px'>might retrieve incomplete or irrelevant passages, leading to subop-<br>timal answer generation. These results underscore the robustness<br>and efficiency of our method, especially for tasks requiring a uni-<br>fied understanding of the source material. While dense retrieval<br>methods such as OpenAI Indexes perform better than sparse re-<br>trieval methods like BM25, both are inherently limited by their<br>dependence on retrieval accuracy and ranking heuristics. Our ap-<br>proach bypasses these challenges, leveraging the long-context ca-<br>pabilities of the Llama 3.1 model to achieve superior performance.</p>
<br><p id='73' data-category='paragraph' style='font-size:16px'>Table 3 compares our CAG approach with standard in-context<br>learning, where the reference text is provided dynamically dur-<br>ing inference, requiring real-time KV-cache computation. The re-<br>sults demonstrate that CAG dramatically reduces generation time,<br>particularly as the reference text length increases. This efficiency<br>stems from preloading the KV-cache, which eliminates the need to<br>process the reference text on the fly.</p>
<br><p id='74' data-category='paragraph' style='font-size:16px'>Moreover, CAG is also faster than traditional RAG systems, as<br>it bypasses the retrieval stage entirely. Unlike RAG, CAG does not<br>require retrieval or reference text input during inference, stream-<br>lining the process and further enhancing efficiency. These advan-<br>tages make CAG an optimal solution for scenarios with extensive<br>reference contexts, offering substantial time savings without com-<br>promising performance.</p>
<p id='75' data-category='paragraph' style='font-size:20px'>4 Conclusion</p>
<br><p id='76' data-category='paragraph' style='font-size:16px'>As long-context LLMs evolve, we present a compelling case for<br>rethinking traditional RAG workflows. While our work empha-<br>sizes eliminating retrieval latency, there is potential for hybrid ap-<br>proaches that combine preloading with selective retrieval. For ex-<br>ample, a system could preload a foundation context and use re-<br>trieval only to augment edge cases or highly specific queries. This<br>would balance the efficiency of preloading with the flexibility of<br>retrieval, making it suitable for scenarios where context complete-<br>ness and adaptability are equally important.</p>
<p id='77' data-category='paragraph' style='font-size:22px'>References</p>
<br><p id='78' data-category='list' style='font-size:14px'>[1] Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai,<br>Jiawei Sun, and Haofen Wang. 2023. Retrieval-augmented generation for large<br>language models: A survey. arXiv preprint arXiv:2312.10997 (2023).<br>[2] Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. 2024.<br>Long Context RAG Performance of Large Language Models. arXiv preprint<br>arXiv:2411.03538 (2024).<br>[3] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir<br>Karpukhin, Naman Goyal, Heinrich K�ttler, Mike Lewis, Wen-tau Yih, Tim Rock-<br>t�schel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp<br>tasks. Advances in Neural Information Processing Systems 33 (2020), 9459-9474.<br>[4] Zhuowan Li, Cheng Li, Mingyang Zhang, Qiaozhu Mei, and Michael Ben-<br>dersky. 2024. Retrieval Augmented Generation or Long-Context LLMs?<br>A Comprehensive Study and Hybrid Approach. In Proceedings of the 2024<br>Conference on Empirical Methods in Natural Language Processing: Industry<br>Track, Franck Dernoncourt, Daniel Preotiuc-Pietro, and Anastasia Shimorina<br>(Eds.). Association for Computational Linguistics, Miami, Florida, US, 881-893.<br>https://bai.ng/00.08/52/1/2024.com/ip/hohohp666<br>[5] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang.<br>2024. TurboRAG: Accelerating Retrieval-Augmented Generation with Pre-<br>computed KV Caches for Chunked Text. arXiv:2410.07590 [cs.CV]<br>https://arxiv.org/abs/2410.07590<br>[6] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016.<br>SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Proceedings<br>of the 2016 Conference on Empirical Methods in Natural Language Processing, Jian<br>Su, Kevin Duh, and Xavier Carreras (Eds.). Association for Computational Lin-<br>guistics, Austin, Texas, 2383-2392. https://doi.org/10.18653/v1/D16-1264<br>[7] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Rus-<br>lan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for</p>
<p id='81' data-category='paragraph' style='font-size:18px'>Diverse, Explainable Multi-hop Question Answering. In Conference on Empirical<br>Methods in Natural Language Processing (EMNLP).</p>
<br><p id='82' data-category='paragraph' style='font-size:18px'>[8] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi.<br>[n. d.]. BERTScore: Evaluating Text Generation with BERT. In International Con-<br>ference on Learning Representations.</p>